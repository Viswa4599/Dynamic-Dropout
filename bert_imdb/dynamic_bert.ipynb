{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T19:30:46.722595Z","iopub.execute_input":"2021-12-15T19:30:46.722911Z","iopub.status.idle":"2021-12-15T19:30:46.737875Z","shell.execute_reply.started":"2021-12-15T19:30:46.722835Z","shell.execute_reply":"2021-12-15T19:30:46.736834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### importing necessaries libraries...","metadata":{}},{"cell_type":"code","source":"import sys\nimport numpy as np\nimport random as rn\nimport pandas as pd\nimport torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\n# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.nn import functional as F\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:49.014157Z","iopub.execute_input":"2021-12-15T19:30:49.014524Z","iopub.status.idle":"2021-12-15T19:30:55.248853Z","shell.execute_reply.started":"2021-12-15T19:30:49.014449Z","shell.execute_reply":"2021-12-15T19:30:55.248147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing seed values to stabilize the outcomes.","metadata":{}},{"cell_type":"code","source":"rn.seed(321)\nnp.random.seed(321)\ntorch.manual_seed(321)\ntorch.cuda.manual_seed(321)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:05.791016Z","iopub.execute_input":"2021-12-15T19:36:05.793134Z","iopub.status.idle":"2021-12-15T19:36:05.802775Z","shell.execute_reply.started":"2021-12-15T19:36:05.79308Z","shell.execute_reply":"2021-12-15T19:36:05.802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data","metadata":{}},{"cell_type":"code","source":"path = '../input/imdb-50k-movie-reviews-test-your-bert/'\n\ntrain_Data = pd.read_csv(path + 'train.csv')\ntest_Data = pd.read_csv(path + 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:32:47.220408Z","iopub.execute_input":"2021-12-15T19:32:47.220703Z","iopub.status.idle":"2021-12-15T19:32:49.126462Z","shell.execute_reply.started":"2021-12-15T19:32:47.220657Z","shell.execute_reply":"2021-12-15T19:32:49.125652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# experimenting here with a sample of dataset, to avoid memory overflow.\ntrain_data = train_Data[:2000]\ntest_data = test_Data[:500]\nval_data = train_Data[:-500]\n\ntrain_data = train_data.to_dict(orient='records')\ntest_data = test_data.to_dict(orient='records')\nval_data = val_data.to_dict(orient='records')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:32:49.12776Z","iopub.execute_input":"2021-12-15T19:32:49.128071Z","iopub.status.idle":"2021-12-15T19:32:49.279832Z","shell.execute_reply.started":"2021-12-15T19:32:49.128023Z","shell.execute_reply":"2021-12-15T19:32:49.27867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mapping sentences with their Labels...","metadata":{}},{"cell_type":"code","source":"train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\ntest_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\nval_texts, val_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), val_data)))\n\nlen(train_texts), len(train_labels), len(test_texts), len(test_labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:32:49.283109Z","iopub.execute_input":"2021-12-15T19:32:49.283435Z","iopub.status.idle":"2021-12-15T19:32:49.312098Z","shell.execute_reply.started":"2021-12-15T19:32:49.283378Z","shell.execute_reply":"2021-12-15T19:32:49.310909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### visualizing one of the sentences from train set","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:32:49.317137Z","iopub.execute_input":"2021-12-15T19:32:49.31738Z","iopub.status.idle":"2021-12-15T19:32:50.354938Z","shell.execute_reply.started":"2021-12-15T19:32:49.317334Z","shell.execute_reply":"2021-12-15T19:32:50.35404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\ntest_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\nval_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], val_texts))\n\nlen(train_tokens), len(test_tokens), len(val_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:32:50.356779Z","iopub.execute_input":"2021-12-15T19:32:50.35724Z","iopub.status.idle":"2021-12-15T19:35:40.790729Z","shell.execute_reply.started":"2021-12-15T19:32:50.35705Z","shell.execute_reply":"2021-12-15T19:35:40.78998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\ntest_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\nval_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, val_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n\ntrain_tokens_ids.shape, test_tokens_ids.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:35:40.792334Z","iopub.execute_input":"2021-12-15T19:35:40.792736Z","iopub.status.idle":"2021-12-15T19:35:44.481552Z","shell.execute_reply.started":"2021-12-15T19:35:40.792576Z","shell.execute_reply":"2021-12-15T19:35:44.480893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y = np.array(train_labels) == 'pos'\ntest_y = np.array(test_labels) == 'pos'\nval_y = np.array(val_labels) == 'pos'\ntrain_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:35:44.482868Z","iopub.execute_input":"2021-12-15T19:35:44.483149Z","iopub.status.idle":"2021-12-15T19:35:44.495399Z","shell.execute_reply.started":"2021-12-15T19:35:44.483103Z","shell.execute_reply":"2021-12-15T19:35:44.494647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now Masking few random IDs from each sentences to remove Biasness from model.","metadata":{}},{"cell_type":"code","source":"train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\ntest_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\nval_masks = [[float(i > 0) for i in ii] for ii in val_tokens_ids]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:35:44.496801Z","iopub.execute_input":"2021-12-15T19:35:44.49732Z","iopub.status.idle":"2021-12-15T19:35:58.713374Z","shell.execute_reply.started":"2021-12-15T19:35:44.497223Z","shell.execute_reply":"2021-12-15T19:35:58.712517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:35:58.714683Z","iopub.execute_input":"2021-12-15T19:35:58.714996Z","iopub.status.idle":"2021-12-15T19:35:59.472751Z","shell.execute_reply.started":"2021-12-15T19:35:58.714949Z","shell.execute_reply":"2021-12-15T19:35:59.471816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:35:59.474396Z","iopub.execute_input":"2021-12-15T19:35:59.474688Z","iopub.status.idle":"2021-12-15T19:36:05.229783Z","shell.execute_reply.started":"2021-12-15T19:35:59.474639Z","shell.execute_reply":"2021-12-15T19:36:05.229038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_predicted = baseline_model.predict(test_texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:05.23111Z","iopub.execute_input":"2021-12-15T19:36:05.231382Z","iopub.status.idle":"2021-12-15T19:36:05.732337Z","shell.execute_reply.started":"2021-12-15T19:36:05.23134Z","shell.execute_reply":"2021-12-15T19:36:05.731494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_labels, baseline_predicted))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:05.737098Z","iopub.execute_input":"2021-12-15T19:36:05.739289Z","iopub.status.idle":"2021-12-15T19:36:05.763761Z","shell.execute_reply.started":"2021-12-15T19:36:05.739233Z","shell.execute_reply":"2021-12-15T19:36:05.763026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Our baseline model is working just fine and yeilding a fair enough score. Now, its time to play Dirty with the \"BERT\".","metadata":{}},{"cell_type":"markdown","source":"# BERT Model\n\n\n### Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is â€“ **BERT is based on the Transformer architecture**.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:05.76774Z","iopub.execute_input":"2021-12-15T19:36:05.771951Z","iopub.status.idle":"2021-12-15T19:36:05.785129Z","shell.execute_reply.started":"2021-12-15T19:36:05.771898Z","shell.execute_reply":"2021-12-15T19:36:05.784159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertBinaryClassifier(nn.Module):\n    def __init__(self, dropout):\n        super(BertBinaryClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout1 = nn.Dropout(0.1)\n        self.linear1 = nn.Linear(768, 1)\n        self.dropout2 = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, tokens, masks=None):\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n        dropout_output1 = self.dropout1(pooled_output)\n        linear_output1 = F.relu(self.linear1(dropout_output1))\n        dropout_output2 = self.dropout2(linear_output1)\n        proba = self.sigmoid(dropout_output2)\n        return proba\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:32.78664Z","iopub.execute_input":"2021-12-15T19:36:32.787104Z","iopub.status.idle":"2021-12-15T19:36:32.79523Z","shell.execute_reply.started":"2021-12-15T19:36:32.787025Z","shell.execute_reply":"2021-12-15T19:36:32.794439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchviz\nfrom torchviz import make_dot\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:34.101817Z","iopub.execute_input":"2021-12-15T19:36:34.102143Z","iopub.status.idle":"2021-12-15T19:36:39.827071Z","shell.execute_reply.started":"2021-12-15T19:36:34.102095Z","shell.execute_reply":"2021-12-15T19:36:39.82587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nbert_clf = BertBinaryClassifier(0.1)\nbert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:36:40.94007Z","iopub.execute_input":"2021-12-15T19:36:40.940392Z","iopub.status.idle":"2021-12-15T19:36:46.85443Z","shell.execute_reply.started":"2021-12-15T19:36:40.940337Z","shell.execute_reply":"2021-12-15T19:36:46.8537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(train_tokens_ids[:3]).to(device)\ny, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\nmake_dot(y, params=dict(list(bert_clf.named_parameters()))).render(\"bert-class\")\nx.shape, y.shape, pooled.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:37:05.583557Z","iopub.execute_input":"2021-12-15T19:37:05.583857Z","iopub.status.idle":"2021-12-15T19:37:08.429574Z","shell.execute_reply.started":"2021-12-15T19:37:05.583803Z","shell.execute_reply":"2021-12-15T19:37:08.428838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = bert_clf(x)\ny.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.083673Z","iopub.status.idle":"2021-12-15T19:30:07.084489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.086083Z","iopub.status.idle":"2021-12-15T19:30:07.086932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y, x, pooled = None, None, None\ntorch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.088428Z","iopub.status.idle":"2021-12-15T19:30:07.08928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tune BERT","metadata":{}},{"cell_type":"code","source":"# Setting hyper-parameters\n\nBATCH_SIZE = 8\nEPOCHS = 10\nlookback = np.ceil(0.05*EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.090642Z","iopub.status.idle":"2021-12-15T19:30:07.091499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n\nval_tokens_tensor = torch.tensor(val_tokens_ids)\nval_y_tensor = torch.tensor(val_y.reshape(-1, 1)).float()\n\ntrain_masks_tensor = torch.tensor(train_masks)\ntest_masks_tensor = torch.tensor(test_masks)\nval_masks_tensor = torch.tensor(val_masks)\n\ntorch.cuda.empty_cache()\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.093054Z","iopub.status.idle":"2021-12-15T19:30:07.093867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n\nval_dataset = TensorDataset(val_tokens_tensor, val_masks_tensor, val_y_tensor)\nval_sampler = SequentialSampler(val_dataset)\nval_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.095366Z","iopub.status.idle":"2021-12-15T19:30:07.096216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(bert_clf.sigmoid.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.097768Z","iopub.status.idle":"2021-12-15T19:30:07.098543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(bert_clf.parameters(), lr=3e-6)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.099962Z","iopub.status.idle":"2021-12-15T19:30:07.100893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\na = torch.zeros(300000000, dtype=torch.int8)\na = a.cuda()\ndel a\ntorch.cuda.empty_cache()\nprint(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.102322Z","iopub.status.idle":"2021-12-15T19:30:07.103187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clearing Cache space for a fresh Model run\ntorch.cuda.empty_cache() \n\ndef set_new_dropout(model,training_losses,validation_losses):\n    print(training_losses)\n    print(validation_losses)\n    avg_val = np.mean([loss for loss in validation_losses[-5:]])\n    avg_train = np.mean([loss for loss in training_losses[-5:]])\n    loss_diff = abs(avg_train-avg_val)\n    percentage_diff = loss_diff/avg_train\n    new_p = percentage_diff\n    if new_p>0.5:\n        new_p = 0.5\n    for name,child in model.named_children():\n            if isinstance(child, torch.nn.Dropout) and name == 'dropout2':\n                child.p = new_p\n\n        \ndef run_bert_classifier(policy):\n    training_losses = []\n    validation_losses = []\n    dropouts  = []\n    BATCH_SIZE = 8\n    EPOCHS = 10\n    best_val_loss = 1000\n    for layer in bert_clf.children():\n        if hasattr(layer, 'reset_parameters'):\n            layer.reset_parameters()\n            \n    for epoch_num in range(EPOCHS):\n        print('Epoch: ', epoch_num + 1)\n        bert_clf.train()\n\n        if policy and epoch_num!=0 and not (epoch_num+2)%lookback:\n            new_p = set_new_dropout(bert_clf,training_losses,validation_losses)\n        \n        for name,child in bert_clf.named_children():\n            if isinstance(child, torch.nn.Dropout) and name == 'dropout2':\n                if epoch_num == 0:\n                    child.p = 0.1\n                rate = child.p\n                print(f\"rate {rate}\")\n    \n        dropouts.append(rate)\n        \n        train_loss = 0\n        for step_num, batch_data in enumerate(train_dataloader):\n            token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n            #print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n            logits = bert_clf(token_ids, masks)\n\n            loss_func = nn.BCELoss()\n\n            batch_loss = loss_func(logits, labels)\n            train_loss += batch_loss.item()\n\n\n            bert_clf.zero_grad()\n            batch_loss.backward()\n\n\n            clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n            optimizer.step()\n        \n        train_loss =  train_loss / (step_num + 1)\n        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE,train_loss))\n        \n        bert_clf.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for step_num, batch_data in enumerate(test_dataloader):\n\n                token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n                logits = bert_clf(token_ids, masks)\n                loss_func = nn.BCELoss()\n                loss = loss_func(logits, labels)\n                val_loss += loss.item()\n                \n        val_loss = val_loss/(step_num+1)\n        if val_loss <= best_val_loss:\n            best_val_loss = val_loss\n            torch.save(bert_clf,'best_model_sofar')\n        print(\"\\r\" + \"{0}/{1} validation loss: {2} \".format(step_num, len(test_data) / BATCH_SIZE, val_loss))\n        training_losses.append(train_loss)\n        validation_losses.append(val_loss)\n        \n    best_model = torch.load('best_model_sofar')\n    return best_model,training_losses,validation_losses,dropouts","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.104752Z","iopub.status.idle":"2021-12-15T19:30:07.10557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policy_model,policy_train,policy_val,varying_dropouts = run_bert_classifier(True)\nmodel,train,val,_ = run_bert_classifier(False)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.107051Z","iopub.status.idle":"2021-12-15T19:30:07.107867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = 10\nfig1 = plt.figure()\nplt.title(\"Loss vs epochs without dynamic dropout\")\nplt.ylim(0,1)\nplt.plot(range(epochs),train,label = 'Train loss')\nplt.plot(range(epochs),val, label = 'Validation loss')\nplt.legend()\nplt.savefig('nopolicy')\nfig1.show()\n\nfig2 = plt.figure()\nplt.title(\"Loss vs epochs with dynamic dropout\")\nplt.ylim(0,1)\nplt.plot(range(epochs),policy_train,label = 'Train loss')\nplt.plot(range(epochs),policy_val, label = 'Validation loss')\nplt.legend()\nplt.savefig('policy')\nfig2.show()\n\n\nfig3 = plt.figure()\nplt.ylim(0,0.5)\nplt.plot(range(epochs),varying_dropouts,color = 'r')\nplt.savefig('dropouts')\nfig3.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.109313Z","iopub.status.idle":"2021-12-15T19:30:07.110075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(test_model):\n    test_model.eval()\n    bert_predicted = []\n    all_logits = []\n    with torch.no_grad():\n        for step_num, batch_data in enumerate(test_dataloader):\n\n            token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n            logits = test_model(token_ids, masks)\n            loss_func = nn.BCELoss()\n            loss = loss_func(logits, labels)\n            numpy_logits = logits.cpu().detach().numpy()\n\n            bert_predicted += list(numpy_logits[:, 0] > 0.5)\n            all_logits += list(numpy_logits[:, 0])\n    np.mean(bert_predicted)\n    print(classification_report(test_y, bert_predicted))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.111428Z","iopub.status.idle":"2021-12-15T19:30:07.11215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.11348Z","iopub.status.idle":"2021-12-15T19:30:07.114306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(policy_model)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T19:30:07.115729Z","iopub.status.idle":"2021-12-15T19:30:07.116565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}